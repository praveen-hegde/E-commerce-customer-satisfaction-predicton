{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Models and saving those for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries....\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data with all created features\n",
    "data = pd.read_csv(\"data_with_advanced_features.csv\")\n",
    "data.drop(\"Unnamed: 0\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding of seller_id\n",
    "label = LabelEncoder()\n",
    "seller = label.fit_transform(data.seller_id)\n",
    "data[\"seller_id\"] = seller\n",
    "\n",
    "#save the encoder\n",
    "filename=\"seller_id_encode.pkl\"\n",
    "pickle.dump(label,open(filename,\"wb\"))\n",
    "\n",
    "#label encoding of product id\n",
    "label = LabelEncoder()\n",
    "product = label.fit_transform(data.product_id)\n",
    "data[\"product_id\"] = product\n",
    "\n",
    "# save the encoder\n",
    "filename=\"product_id_encode.pkl\"\n",
    "pickle.dump(label,open(filename,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Creating binary classifier system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating class labels\n",
    "binary = []\n",
    "for i in range(len(data)):\n",
    "    if data.review_score[i]==5:\n",
    "        binary.append(1)\n",
    "    else:\n",
    "        binary.append(0)\n",
    "        \n",
    "data[\"binary_target\"] = binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable is review_score\n",
    "Y = data[\"binary_target\"]\n",
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split with test size 25% and 75% of data as train\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.25,stratify=Y,random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#payment_type \n",
    "vec = CountVectorizer()\n",
    "\n",
    "vec.fit(x_train[\"payment_type\"].values)\n",
    "\n",
    "x_tr_pay_type = vec.transform(x_train.payment_type.values)\n",
    "x_te_pay_type = vec.transform(x_test.payment_type.values)\n",
    "\n",
    "#save as pickle file\n",
    "filename = \"count_vect_payment_1.pkl\"\n",
    "pickle.dump(vec,open(filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order_item_id \n",
    "x_train.order_item_id = x_train.order_item_id.astype(str)\n",
    "x_test.order_item_id = x_test.order_item_id.astype(str)\n",
    "\n",
    "vec = CountVectorizer(vocabulary=range(1,22))\n",
    "\n",
    "vec.fit(x_train[\"order_item_id\"])\n",
    "\n",
    "x_tr_id = vec.transform(x_train.order_item_id)\n",
    "x_te_id = vec.transform(x_test.order_item_id)\n",
    "\n",
    "#save as pickle file\n",
    "filename = \"count_vect_item_1.pkl\"\n",
    "pickle.dump(vec,open(filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product_category_name\n",
    "vec = CountVectorizer()\n",
    "\n",
    "vec.fit(x_train[\"product_category_name\"].values)\n",
    "\n",
    "x_tr_cat = vec.transform(x_train.product_category_name.values)\n",
    "x_te_cat = vec.transform(x_test.product_category_name.values)\n",
    "\n",
    "#save as pickle file\n",
    "filename = \"count_vect_cat_1.pkl\"\n",
    "pickle.dump(vec,open(filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_same_state = x_train.same_state.values.reshape(-1,1)\n",
    "x_te_same_state = x_test.same_state.values.reshape(-1,1)\n",
    "\n",
    "x_tr_same_city = x_train.same_city.values.reshape(-1,1)\n",
    "x_te_same_city = x_test.same_city.values.reshape(-1,1)\n",
    "\n",
    "x_tr_late_shipping = x_train.late_shipping.values.reshape(-1,1)\n",
    "x_te_late_shipping = x_test.late_shipping.values.reshape(-1,1)\n",
    "\n",
    "x_tr_high_freight = x_train.high_freight.values.reshape(-1,1)\n",
    "x_te_high_freight = x_test.high_freight.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data to be standardized\n",
    "tr = x_train[[\"payment_sequential\",\"payment_installments\",\"payment_value\",\"seller_id\",\"product_id\",\"seller_share\",\"bu_share\",\n",
    "              \"bs_share\",\"cust_share\",\n",
    "          \"lat_customer\",\"lng_customer\",\"lat_seller\",\"lng_seller\",\"product_name_lenght\",\"product_description_lenght\",\n",
    "           \"product_photos_qty\",\"product_weight_g\",\"size\",\"price\",\"delivery_day\",\"delivery_date\",\"delivery_month\",\n",
    "              \"delivery_hour\",\"purchased_day\",\"purchased_date\",\"purchased_month\",\"purchased_hour\",\"num_of_customers_for_seller\",\n",
    "              \"num_of_sellers_for_cust\",\"total_order_for_seller\",\n",
    "           \"freight_value\",\"estimated_time\",\"actual_time\",\"diff_actual_estimated\",\"diff_purchased_approved\",\n",
    "           \"diff_purchased_courrier\",\"distance\",\"speed\",\"similarity\",\"similarity_using_cat\"]]\n",
    "\n",
    "te = x_test[[\"payment_sequential\",\"payment_installments\",\"payment_value\",\"seller_id\",\"product_id\",\"seller_share\",\"bu_share\",\n",
    "              \"bs_share\",\"cust_share\",\n",
    "          \"lat_customer\",\"lng_customer\",\"lat_seller\",\"lng_seller\",\"product_name_lenght\",\"product_description_lenght\",\n",
    "           \"product_photos_qty\",\"product_weight_g\",\"size\",\"price\",\"delivery_day\",\"delivery_date\",\"delivery_month\",\n",
    "              \"delivery_hour\",\"purchased_day\",\"purchased_date\",\"purchased_month\",\"purchased_hour\",\"num_of_customers_for_seller\",\n",
    "              \"num_of_sellers_for_cust\",\"total_order_for_seller\",\n",
    "           \"freight_value\",\"estimated_time\",\"actual_time\",\"diff_actual_estimated\",\"diff_purchased_approved\",\n",
    "           \"diff_purchased_courrier\",\"distance\",\"speed\",\"similarity\",\"similarity_using_cat\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScaler()\n",
    "\n",
    "norm.fit(tr.values)\n",
    "\n",
    "x_tr_num = norm.transform(tr.values)\n",
    "x_te_num = norm.transform(te.values)\n",
    "\n",
    "#save as pickle file\n",
    "filename = \"std_num_1.pkl\"\n",
    "pickle.dump(norm,open(filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal stacking of all the features\n",
    "train = hstack((x_tr_pay_type,x_tr_id,x_tr_cat,x_tr_num,x_tr_same_state,\n",
    "                   x_tr_same_city,x_tr_late_shipping,x_tr_high_freight)).toarray()\n",
    "\n",
    "test = hstack((x_te_pay_type,x_te_id,x_te_cat,x_te_num,x_te_same_state,\n",
    "                 x_te_same_city,x_te_late_shipping,x_te_high_freight)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the index of target variable\n",
    "y_trains = y_train.reset_index()\n",
    "y_train = y_trains[\"binary_target\"]\n",
    "\n",
    "y_tests = y_test.reset_index()\n",
    "y_test = y_tests[\"binary_target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight='balanced')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param = 0.01\n",
    "model = LogisticRegression(C=best_param,class_weight=\"balanced\")\n",
    "model.fit(train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the logistic model as pickle file\n",
    "filename = \"binary_model.pkl\"\n",
    "pickle.dump(model,open(filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Ensemble for (1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data with all created features\n",
    "data = pd.read_csv(\"data_with_advanced_features.csv\")\n",
    "data.drop(\"Unnamed: 0\", inplace=True, axis=1)\n",
    "\n",
    "#label encoding of seller_id\n",
    "label = LabelEncoder()\n",
    "seller = label.fit_transform(data.seller_id)\n",
    "data[\"seller_id\"] = seller\n",
    "\n",
    "filename = \"seller_encode_2.pkl\"\n",
    "pickle.dump(label,open(filename,\"wb\"))\n",
    "\n",
    "#label encoding of product id\n",
    "label = LabelEncoder()\n",
    "product = label.fit_transform(data.product_id)\n",
    "data[\"product_id\"] = product\n",
    "\n",
    "filename = \"product_encode_2.pkl\"\n",
    "pickle.dump(label,open(filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"review_score\"]!=5]\n",
    "Y = data[\"review_score\"]\n",
    "X = data\n",
    "\n",
    "######### train test split with test size 25% and 75% of data as train ##############\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=10)\n",
    "###########################################################################################################\n",
    "\n",
    "######## payment_type ##########\n",
    "vec = CountVectorizer()\n",
    "vec.fit(x_train[\"payment_type\"].values)\n",
    "x_tr_pay_type = vec.transform(x_train.payment_type.values)\n",
    "x_te_pay_type = vec.transform(x_test.payment_type.values)\n",
    "\n",
    "# save as pickle file\n",
    "filename = \"countvec_pay_2.pkl\"\n",
    "pickle.dump(vec,open(filename,\"wb\"))\n",
    "\n",
    "\n",
    "###### order_item_id ###########\n",
    "x_train.order_item_id = x_train.order_item_id.astype(str)\n",
    "x_test.order_item_id = x_test.order_item_id.astype(str)\n",
    "\n",
    "vec = CountVectorizer(vocabulary=range(1,22))\n",
    "vec.fit(x_train[\"order_item_id\"])\n",
    "x_tr_id = vec.transform(x_train.order_item_id)\n",
    "x_te_id = vec.transform(x_test.order_item_id)\n",
    "\n",
    "# save as pickle file\n",
    "filename = \"countvec_item_2.pkl\"\n",
    "pickle.dump(vec,open(filename,\"wb\"))\n",
    "\n",
    "######### product_category_name ############\n",
    "vec = CountVectorizer()\n",
    "vec.fit(x_train[\"product_category_name\"].values)\n",
    "x_tr_cat = vec.transform(x_train.product_category_name.values)\n",
    "x_te_cat = vec.transform(x_test.product_category_name.values)\n",
    "\n",
    "# save as pickle file\n",
    "filename = \"countvec_cat_2.pkl\"\n",
    "pickle.dump(vec,open(filename,\"wb\"))\n",
    "\n",
    "########## Binary features #####################\n",
    "x_tr_same_state = x_train.same_state.values.reshape(-1,1)\n",
    "x_te_same_state = x_test.same_state.values.reshape(-1,1)\n",
    "\n",
    "x_tr_same_city = x_train.same_city.values.reshape(-1,1)\n",
    "x_te_same_city = x_test.same_city.values.reshape(-1,1)\n",
    "\n",
    "x_tr_late_shipping = x_train.late_shipping.values.reshape(-1,1)\n",
    "x_te_late_shipping = x_test.late_shipping.values.reshape(-1,1)\n",
    "\n",
    "x_tr_high_freight = x_train.high_freight.values.reshape(-1,1)\n",
    "x_te_high_freight = x_test.high_freight.values.reshape(-1,1)\n",
    "\n",
    "################################################################################\n",
    "############# data to be standardized #########################################\n",
    "tr = x_train[[\"payment_sequential\",\"payment_installments\",\"payment_value\",\"seller_id\",\"product_id\",\"seller_share\",\"bu_share\",\n",
    "              \"bs_share\",\"cust_share\",\n",
    "          \"lat_customer\",\"lng_customer\",\"lat_seller\",\"lng_seller\",\"product_name_lenght\",\"product_description_lenght\",\n",
    "           \"product_photos_qty\",\"product_weight_g\",\"size\",\"price\",\"delivery_day\",\"delivery_date\",\"delivery_month\",\n",
    "              \"delivery_hour\",\"purchased_day\",\"purchased_date\",\"purchased_month\",\"purchased_hour\",\"num_of_customers_for_seller\",\n",
    "              \"num_of_sellers_for_cust\",\"total_order_for_seller\",\n",
    "           \"freight_value\",\"estimated_time\",\"actual_time\",\"diff_actual_estimated\",\"diff_purchased_approved\",\n",
    "           \"diff_purchased_courrier\",\"distance\",\"speed\",\"similarity\",\"similarity_using_cat\"]]\n",
    "\n",
    "te = x_test[[\"payment_sequential\",\"payment_installments\",\"payment_value\",\"seller_id\",\"product_id\",\"seller_share\",\"bu_share\",\n",
    "              \"bs_share\",\"cust_share\",\n",
    "          \"lat_customer\",\"lng_customer\",\"lat_seller\",\"lng_seller\",\"product_name_lenght\",\"product_description_lenght\",\n",
    "           \"product_photos_qty\",\"product_weight_g\",\"size\",\"price\",\"delivery_day\",\"delivery_date\",\"delivery_month\",\n",
    "              \"delivery_hour\",\"purchased_day\",\"purchased_date\",\"purchased_month\",\"purchased_hour\",\"num_of_customers_for_seller\",\n",
    "              \"num_of_sellers_for_cust\",\"total_order_for_seller\",\n",
    "           \"freight_value\",\"estimated_time\",\"actual_time\",\"diff_actual_estimated\",\"diff_purchased_approved\",\n",
    "           \"diff_purchased_courrier\",\"distance\",\"speed\",\"similarity\",\"similarity_using_cat\"]]\n",
    "\n",
    "\n",
    "norm = StandardScaler()\n",
    "\n",
    "norm.fit(tr.values)\n",
    "\n",
    "x_tr_num = norm.transform(tr.values)\n",
    "x_te_num = norm.transform(te.values)\n",
    "\n",
    "# save as pickle file\n",
    "filename = \"std_num_2.pkl\"\n",
    "pickle.dump(norm,open(filename,\"wb\"))\n",
    "#################################################################################################################\n",
    "\n",
    "#horizontal stacking of all the features\n",
    "train = hstack((x_tr_pay_type,x_tr_id,x_tr_cat,x_tr_num,x_tr_same_state,\n",
    "                   x_tr_same_city,x_tr_late_shipping,x_tr_high_freight)).toarray()\n",
    "\n",
    "test = hstack((x_te_pay_type,x_te_id,x_te_cat,x_te_num,x_te_same_state,\n",
    "                 x_te_same_city,x_te_late_shipping,x_te_high_freight)).toarray()\n",
    "\n",
    "#reset the index of target variable\n",
    "y_trains = y_train.reset_index()\n",
    "y_train = y_trains[\"review_score\"]\n",
    "\n",
    "y_tests = y_test.reset_index()\n",
    "y_test = y_tests[\"review_score\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Custom ensemble with Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ensemble(x_tr,y_tr,x_te,n_estimators,estimator,meta_clf):\n",
    "    \"\"\"This function creates the custom ensemble model and returns predicted  target variable of test set\"\"\"\n",
    "    \n",
    "    ########### SPlitting train data into 50-50 as d1 and d2 ############\n",
    "    kf = StratifiedKFold(n_splits=2)\n",
    "    \n",
    "    d1 = x_tr[list(kf.split(x_tr,y_tr))[1][0]]\n",
    "    d1_y = y_tr[list(kf.split(x_tr,y_tr))[1][0]]\n",
    "\n",
    "    d2 = x_tr[list(kf.split(x_tr,y_tr))[1][1]]\n",
    "    d2_y = y_tr[list(kf.split(x_tr,y_tr))[1][1]]\n",
    "    #####################################################################\n",
    "    d1_y = np.array(d1_y)\n",
    "    d2_y = np.array(d2_y)\n",
    "    #####################################################################\n",
    "    ### Creating base learners and training them using samples of d1 ####\n",
    "    \n",
    "    models=[]\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        ind = np.random.choice(19387,size=(20000),replace=True)\n",
    "        sample = d1[ind]\n",
    "        sample_y = d1_y[ind]  \n",
    "        \n",
    "        estimator.fit(sample,sample_y)\n",
    "        models.append(estimator)\n",
    "    \n",
    "    # save as pickle file\n",
    "    filename=\"base_models.pkl\"\n",
    "    pickle.dump(models,open(filename,\"wb\"))\n",
    "    ########### Predictions from base learners for d2 set ###############\n",
    "    predictions = []\n",
    "    for model in models: \n",
    "        \n",
    "        pred = model.predict(d2)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "    predictions = np.array(predictions).reshape(-1,n_estimators)\n",
    "    \n",
    "    ########## meta classifier on predictions of base learners ##########\n",
    "    \n",
    "    meta_clf.fit(predictions,d2_y)\n",
    "    \n",
    "    # save as pickle file\n",
    "    filename=\"meta_clf.pkl\"\n",
    "    pickle.dump(meta_clf,open(filename,\"wb\"))\n",
    "    #####################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and saving the models with best hyperparameter n_estimator=150\n",
    "best_n = 150\n",
    "train_pred,test_pred,d2_y = custom_ensemble(train,y_train,test,best_n,LogisticRegression(class_weight=\"balanced\"),\n",
    "                                       LogisticRegression(class_weight=\"balanced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Hence We have saved all the objects and models that are necessary for further use/deployment. \n",
    "    \n",
    "We have selected the best performing model as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
